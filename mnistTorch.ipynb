{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_data = torchvision.datasets.MNIST(root='../data',download=False,transform=transform,train=True)\n",
    "#train_loader = torch.utils.data.DataLoader(train_data,batch_size=4,shuffle=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "])),batch_size=4,shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('data', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "])),batch_size=4,shuffle=True)\n",
    "j=0\n",
    "for i,data in enumerate(train_loader,0):\n",
    "    if(j<1):\n",
    "        j+=1\n",
    "        #print (\"i=\",i,\"\\n\")\n",
    "        print (data[0])\n",
    "\n",
    "#NOTE:: keep a \"raw\" folder and \"processed\" folder. Place mnist data in raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=('0','1','2','3','4','5','6','7','8','9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,5)\n",
    "        self.conv2 = nn.Conv2d(10,20,5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "        #self.fc3 = nn.Linear(84,10)\n",
    "    def forward(self,x):\n",
    "        x = nnf.max_pool2d(nnf.relu(self.conv1(x)),(2,2))\n",
    "        x = nnf.max_pool2d(nnf.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = nnf.relu(self.fc1(x))\n",
    "        x = nnf.dropout(x,self.training)\n",
    "        #x = nnf.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return nnf.log_softmax(x,dim=1)\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "for epoch in range(2):\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(train_loader,0):\n",
    "        inputs,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        print (type(inputs))\n",
    "        print (inputs.shape)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss +=loss.item()\n",
    "        if(i>10):\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:      7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7])\n",
      "Predicted:      7\n"
     ]
    }
   ],
   "source": [
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print (predicted)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy:: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"/home/raghu/Desktop/infilect/week1/myModelNormalized.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
